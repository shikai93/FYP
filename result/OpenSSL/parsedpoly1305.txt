Copyright 2015-2016 The OpenSSL Project Authors. All Rights Reserved. Licensed under the OpenSSL license (the "License"). You may not use this file except in compliance with the License. You can obtain a copy in the file LICENSE in the source distribution or at https://www.openssl.org/source/license.html

www.openssl.org/source/license.html

License

Copyright 2015-2016 The OpenSSL Project Authors. All Rights Reserved. Licensed under the OpenSSL license (the "License"). You may not use this file except in compliance with the License. You can obtain a copy in the file LICENSE in the source distribution or at https://www.openssl.org/source/license.html

pick 32-bit unsigned integer in little endian order

Implementations can be classified by amount of significant bits in words making up the multi-precision value, or in other words radix or base of numerical representation, e.g. base 2^64, base 2^32, base 2^26. Complementary characteristic is how wide is the result of multiplication of pair of digits, e.g. it would take 128 bits to accommodate multiplication result in base 2^64 case. These are used interchangeably. To describe implementation that is. But interface is designed to isolate this so that low-level primitives implemented in assembly can be self-contained/self-coherent.

Even though there is __int128 reference implementation targeting 64-bit platforms provided below, it's not obvious that it's optimal choice for every one of them. Depending on instruction set overall amount of instructions can be comparable to one in __int64 implementation. Amount of multiplication instructions would be lower, but not necessarily overall. And in out-of-order execution context, it is the latter that can be crucial... On related note. Poly1305 author, D. J. Bernstein, discusses and provides floating-point implementations of the algorithm in question. It made a lot of sense by the time of introduction, because most then-modern processors didn't have pipelined integer multiplier. [Not to mention that some had non-constant timing for integer multiplications.] Floating-point instructions on the other hand could be issued every cycle, which allowed to achieve better performance. Nowadays, with SIMD and/or out-or-order execution, shared or even emulated FPU, it's more complicated, and floating-point implementation is not necessarily optimal choice in every situation, rather contrary... <appro@openssl.org>

poly1305_blocks processes a multiple of POLY1305_BLOCK_SIZE blocks of |inp| no longer than |len|. Behaviour for |len| not divisible by block size is unspecified in general case, even though in reference implementation the trailing chunk is simply ignored. Per algorithm specification, every input block, complete or last partial, is to be padded with a bit past most significant byte. The latter kind is then padded with zeros till block size. This last partial block padding is caller()'s responsibility, and because of this the last partial block is always processed with separate call with |len| set to POLY1305_BLOCK_SIZE and |padbit| to 0. In all other cases |padbit| should be set to 1 to perform implicit padding with 128th bit. poly1305_blocks does not actually check for this constraint though, it's caller()'s responsibility to comply. () In the context "caller" is not application code, but higher level Poly1305_ from this very module, so that quirks are handled locally.

Type-agnostic "rip-off" from constant_time_locl.h

pick 32-bit unsigned integer in little endian order

store a 32-bit unsigned integer in little endian

h = 0

r &= 0xffffffc0ffffffc0ffffffc0fffffff

h += m[i]

padbit can be zero only when original len was POLY1306_BLOCK_SIZE, but we don't check

h = r "%" p, where "%" stands for "partial remainder"

last reduction step:

a) h2:h0 = h2<<128 + d1<<64 + d0

b) (h2:h0 += (h2:h0>>130) 5) %= 2^130

Occasional overflows to 3rd bit of h2 are taken care of "naturally". If after this point we end up at the top of this loop, then the overflow bit will be accounted for in next iteration. If we end up in poly1305_emit, then comparison to modulus below will still count as "carry into 131st bit", so that properly reduced value will be picked in conditional move.

compare to modulus by computing h + -p

if there was carry into 131st bit, h1:h0 = g1:g0

mac = (h + nonce) % (2^128)

store a 32-bit unsigned integer in little endian

h = 0

r &= 0xffffffc0ffffffc0ffffffc0fffffff

h += m[i]

h = r "%" p, where "%" stands for "partial remainder"

last reduction step:

a) h4:h0 = h4<<128 + d3<<96 + d2<<64 + d1<<32 + d0

b) (h4:h0 += (h4:h0>>130) 5) %= 2^130

Occasional overflows to 3rd bit of h4 are taken care of "naturally". If after this point we end up at the top of this loop, then the overflow bit will be accounted for in next iteration. If we end up in poly1305_emit, then comparison to modulus below will still count as "carry into 131st bit", so that properly reduced value will be picked in conditional move.

compare to modulus by computing h + -p

if there was carry into 131st bit, h3:h0 = g3:g0

mac = (h + nonce) % (2^128)

Unlike reference poly1305_init assembly counterpart is expected to return a value: non-zero if it initializes ctx->func, and zero otherwise. Latter is to simplify assembly in cases when there no multiple code paths to switch between.

This "eclipses" poly1305_blocks and poly1305_emit, but it's conscious choice imposed by -Wshadow compiler warnings.

As documented, poly1305_blocks is never called with input longer than single block and padbit argument set to 0. This property is fluently used in assembly modules to optimize padbit handling on loop boundary.

Still not enough data to process a block.

pad bit

zero out the state

www.openssl.org/source/license.html

License

internal/poly1305.h

poly1305_local.h

s not obvious that it

t have pipelined integer multiplier. * [Not to mention that some had non-constant timing for integer * multiplications.] Floating-point instructions on the other hand could * be issued every cycle, which allowed to achieve better performance. * Nowadays, with SIMD and/or out-or-order execution, shared or * even emulated FPU, it

s responsibility, and because of this the last partial * block is always processed with separate call with |len| set to * POLY1305_BLOCK_SIZE and |padbit| to 0. In all other cases |padbit| * should be set to 1 to perform implicit padding with 128th bit. * poly1305_blocks does not actually check for this constraint though, * it

s responsibility to comply. * * (*) In the context "caller" is not application code, but higher * level Poly1305_* from this very module, so that quirks are * handled locally. */ static void poly1305_blocks(void *ctx, const unsigned char *inp, size_t len, u32 padbit); /* * Type-agnostic "rip-off" from constant_time_locl.h */ # define CONSTANT_TIME_CARRY(a,b) ( \ (a ^ ((a ^ b) | ((a - b) ^ b))) >> (sizeof(a) * 8 - 1) \ ) # if !defined(PEDANTIC) && \ (defined(__SIZEOF_INT128__) && __SIZEOF_INT128__==16) && \ (defined(__SIZEOF_LONG__) && __SIZEOF_LONG__==8) typedef unsigned long u64; typedef unsigned __int128 u128; typedef struct { u64 h[3]; u64 r[2]; } poly1305_internal; /* pick 32-bit unsigned integer in little endian order */ static u64 U8TOU64(const unsigned char *p) { return (((u64)(p[0] & 0xff)) | ((u64)(p[1] & 0xff) << 8) | ((u64)(p[2] & 0xff) << 16) | ((u64)(p[3] & 0xff) << 24) | ((u64)(p[4] & 0xff) << 32) | ((u64)(p[5] & 0xff) << 40) | ((u64)(p[6] & 0xff) << 48) | ((u64)(p[7] & 0xff) << 56)); } /* store a 32-bit unsigned integer in little endian */ static void U64TO8(unsigned char *p, u64 v) { p[0] = (unsigned char)((v) & 0xff); p[1] = (unsigned char)((v >> 8) & 0xff); p[2] = (unsigned char)((v >> 16) & 0xff); p[3] = (unsigned char)((v >> 24) & 0xff); p[4] = (unsigned char)((v >> 32) & 0xff); p[5] = (unsigned char)((v >> 40) & 0xff); p[6] = (unsigned char)((v >> 48) & 0xff); p[7] = (unsigned char)((v >> 56) & 0xff); } static void poly1305_init(void *ctx, const unsigned char key[16]) { poly1305_internal *st = (poly1305_internal *) ctx; /* h = 0 */ st->h[0] = 0; st->h[1] = 0; st->h[2] = 0; /* r &= 0xffffffc0ffffffc0ffffffc0fffffff */ st->r[0] = U8TOU64(&key[0]) & 0x0ffffffc0fffffff; st->r[1] = U8TOU64(&key[8]) & 0x0ffffffc0ffffffc; } static void poly1305_blocks(void *ctx, const unsigned char *inp, size_t len, u32 padbit) { poly1305_internal *st = (poly1305_internal *)ctx; u64 r0, r1; u64 s1; u64 h0, h1, h2, c; u128 d0, d1; r0 = st->r[0]; r1 = st->r[1]; s1 = r1 + (r1 >> 2); h0 = st->h[0]; h1 = st->h[1]; h2 = st->h[2]; while (len >= POLY1305_BLOCK_SIZE) { /* h += m[i] */ h0 = (u64)(d0 = (u128)h0 + U8TOU64(inp + 0)); h1 = (u64)(d1 = (u128)h1 + (d0 >> 64) + U8TOU64(inp + 8)); /* * padbit can be zero only when original len was * POLY1306_BLOCK_SIZE, but we don

%

%

partial remainder

naturally

carry * into 131st bit

%

%

partial remainder

naturally

carry * into 131st bit

eclipses

0xff))

0xff)

0xff)

0xff)

0xff))

0xff)

0xff)

0xff)

0xff)

0xff)

0xff)

0xff)

0xff);

0xff);

0xff);

0xff);

0xff);

0xff);

0xff);

0xff);

0xffffffc0ffffffc0ffffffc0fffffff

0x0ffffffc0fffffff;

0x0ffffffc0ffffffc;

0xff);

0xff);

0xff);

0xff);

0xffffffc0ffffffc0ffffffc0fffffff

0x0fffffff;

0x0ffffffc;

0x0ffffffc;

0x0ffffffc;

